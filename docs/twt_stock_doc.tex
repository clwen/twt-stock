\title{CSC 2511 Project: Predicting Stock Prices using Opnion Mining and Setiment Analysis of Twitter Data}
\author{
    Chung-Lin Wen \\
    Department of Computer Science \\
    University of Toronto
}
\date{\today}

\documentclass[12pt]{article}
\fontfamily{ptm}

\begin{document}
\maketitle

\input{defs}

\section{Introduction}
The rapid populization of social networks made it possible to understand the opinion directly from consumers, without doing additional polls or surveys. The opnion from consumer is important because it reflects how they evaluate the product. It is highly possible they will shop from the same company only if they have an overall positive experience about the products/services they paid, otherwise, they might turn to other alternatives. Additionally, the public opnion will also influence the shopping behavior of their friends, family, collegues, whoever their connection is, which is especially true on the social networks. All of these buying decisions will eventually generate revenue or incur losses for the companies, depending on whether their consumers are happy or not. Hence, it is a reasonable assumption that the performance of a company can be predicted by consumer opinions to some extent, which is especially true if we have sufficient amount of data. 

Consequently, we propose to verify that the performance of a company can be predicted from opinions collected from social networks. There are several alternatives for measuring the success of a company, such as net cash flow, turnover growth, gross margin, etc. However, most of them are biased by size, industry, subfield they are working on. In contrast, stock price doesn't suffer from these biases. In addition, almost all the metrics mentioned before will eventually reflected on the stock price.

To verify our assumption, we will start with a few companies that are widely discussed over the web. Besides, ideally, its revenue should primarily depend on a few key products, which means we can use a few keywords to collect the data we need. For instance, \#iphone4s, \#iphone4, \#iphone3, \#ipad, \#macbookair, etc. Apple Incorporation, for instance, is one of the companies we will start with, since it meets the condition we discussed above. It is widely discussed on social network. If one query 'Apple' or 'iPhone' on Twitter, most of the tweets are published within several minutes, indicates the frequency of tweets mentioning it is fairly high. In contrast, if one query 'Baidu' on Twitter, there are tweets older than one hour even in the first page.

For the social network platform we will use, we favor Twitter as our major data source for the following reasons. First, the amount of data is huge enough for our usage. Second, its microblogging nature provide sentence level texts that focus on usually a single idea (I love \#iPhone4S coz I had a lot of fun playing with \#Siri), thus reducing the noise we have to deal with.

After collecting the data, we will classify the tweets into positive, neutral (or irrelevant), negative as the measurements of consumer evaluation. After that, we will study the relationship between this evaluation and the stock price of corresponding companies using statistical techniques.

\section{Related Works}
For its wide and valuable usage, sentiment analysis, which is to classify the polarity (positive, negative) given a document or a sentence, has been widely studied yet still an extremely challenging problem. Pang and Lee \cite{Pang:08} did a comprehensive survey about the state-of-the-art techniques in the sentimental area. Liu \cite{Liu:10} summarizes the theory and techniques of the subfield for the purpose of teaching and learning.

Turney \etal~\cite{Turney:02} pioneered in the field by firstly address the problem. He first extract phrases that might contain subjective evaluation using its POS tags, then apply PMI-IR algorithm \cite{Turney:01} to determine the semantic orientation by measuring the similarity of the extracted phrases and "excellent", "poor" (regarded as positive and negative, respectively, in this paper). Finally, the polarity of a document is given by averaging the semantic orientations of all the extracted phrases.

In contrast, rather than using predefined bags of words for polarity, Pang \etal~\cite{Pang:02} approach the problem using features that are prior-knowledge-free. They use unigrams and bigrams extracted from corpus according to the frequencies then apply the standard machine learning classifiers, which including Naive Bayes, Maximum Entropy and Support Vector Machines (SVM). They concludes that using the presence of unigrams and using SVM as classfier yields the best accuracy (82.9\%).

Besides polarity, detection of opinion strength is also made posible by further research. Thelwall \etal~\cite{Thelwall:10} proposed to use a dictionary of sentiment words with associated strength measures as one of its feature to detect the opinion strength. Repeated letters, punctuation, emoticons are also used to make the system suitable for informal text.

Since the popularity of social networks, especially the microblogging has grown quickly in the recent years, there are several systems utilize these platforms as the corpus of sentiment analysis. Go \etal~\cite{Go:09} first experiment sentiment analysis on Twitter, the most popular microblogging platform. They basically follow the algorithm used as Pang \etal~\cite{Pang:02}. Unigram, Bigram, POS tags are used as features; Bayes, SVM, Maximum Entropy are used as classifier. They report that the combination of unigram and Bayes with mutual information feature selection method yields the best result, which is slightly different from Pang \etal~\cite{Pang:02}

Pak and Paroubek \cite{Pak:10} conduct a two-phase classification to improve the accuracy. Tweets are first divide to objective and subjective, then subjective tweets are further divide to positive and negative. For training data, they use pre-defined emoticons to get the subjective data, while retrieve tweets from popular newspaper accounts as objective data. In terms of features and classifiers, they follow a similar setup as Pang \etal~\cite{Pang:02} but also includes trigrams as one of the feature. They conclude that bigram yields the best result since it maintains a good balance between coverage and also able to catch sentiment expressions such as negation.

Barbosa and Feng \cite{Barbosa:10} propose propose new features to increase the accuracy of Twitter sentiment analysis. They propose the use of two additional sets of features. One of the sets is the meta features. In addition to POS tags, a word list with pre-defined polarity and subjectivity is used. Note that if the word is preceded by negative expression (e.g., not, never), the polarity will be reversed. The other set they propose is the use of tweets syntax feature, such as retweet, hashtags, reply, link.

Agarwal \etal~\cite{Agarwal:11} claimed they have ahiceved a better result by incorporating the tree kernel model, which is a tree of features such as stop words, target, URL, English words. They compared the tree kernel based model with unigram model and feature-based model from previous work and conclude that the tree kernel model outperforms the other two. Besides, they claimed that using random sampled streaming tweets instead of that searched by keywords introduces fewer biases.

There are different applications on data mining of Twitter or other social networks. Petrovic \etal~\cite{Petrovic:10} proposed a system to detect new event on Twitter, on which news propagate even quicker than the traditional news medias.

There are also applications for sentiment analysis, although not necessarily using Twitter data as a corpus. Besides only indicating the review is positive or negative, Kim \etal~\cite{Kim:06} proposed a system that also extract the reasons behind these positive and negative opnions by an maximum entropy model.

Blair-Goldensohn \etal~\cite{Blair:08} propose to incorporate the readily available information other than text itself to produce a better result. They have also integrated a summary of customer review, which includes different aspects (e.g., food, decor, service, etc in a restaurant) and the corresponding reasons customers love or hates a specific store or product.

Among all the applications, we are most interested about applications that utilize sentiment analysis or opnion mining to predict some metrics in the physical world. Asur and Huberman \cite{Asur:10} used Twitter data to predict the box-office revenues for movies. They study 24 movies released between November 2009 and February 2010 and conclude that the average tweet-rates (published tweets per hour) is statiscally correalated with its box-office revenue. They also report that sentiment content can improve the correalation although not significant as tweet-rates themselves.

There are also financial applications using sentiment analysis for predicting stock prices. Schumaker \etal~\cite{Schumaker:2009} propose a system, AZFinText, which can predict the stock price of a company that mentioned by a financial news. They asuume financial news will have impact on the decision made within stock buyers, and it will have immediate effect so that the price after 20 minutes the news is released can be predicted. They implemented the SVR Sequential Minimal Optimization function through WEKA \cite{Witten:2005}, while using proper nouns and results of sentiment analysis as features. They conclude that both subjectivety and polarity contribute to the accuracy of stock prices prediction.

There are also private companies such as Topsy Labs \cite{Topsy} doing the same investigation as we do. However, they didn't publish the algorithms and methodology they used.

\section{Experiment Setup}
\subsection{Overview}
In this section, we will describe how we set up our experiment. Our experiment contains mainly two parts: sentiment analysis for tweets and statistical inference of relationship between sentiment analysis and stock prices. We first do a two-phase (subjectivity and then polarity) sentiment analysis then we investigate whether the ratio of sentiment analysis are positive correalated to stock prices or not by using statistical method. For sentiment analysis, it can be further decompose into the following modules: data collection, preprocessing, feature extraction and classification. 

We will detail each module by a subsection in the following of this section. In subsection \ref{data-collection} we will discuss how our training data and test data are defined and collecetd. In subsection \ref{preprocessing} we will discuss what kind of preprocessing we will apply to our data. Feature definition and selection will be discussed in subsection \ref{feature-extraction}. Then we detail the classification process in subsection \ref{classification}. Finally, we discuss the statiscal method to be used for relationship between sentiment analysis and stock prices in subsection \ref{statistical-inference}.

\subsection{Data Collection}\label{data-collection}
Our data can be divided into the following portions: test data, which contain tweets about products or companies we are interested; training data, which can be further divide to positive training data, negative training data, objective training data. We will first discuss the methodolgy we collect the data, then define our test data and training data subsequently.

\subsubsection{Methodology}
For the reasons stated before, we will mainly use Twitter as our primary corpus. Ideally, we would like to collect tweets back to one or two years to catch the trends that whenever a popular comes to the market, the stock prices soars (such as iPhone4), or reversely, whenever a product comes to the market but not fully satisfied consumer's expection, the stock price plump (such as iPhone4S). 

However, Since Twitter API only allow external users to parse the tweets no longer than a week from the time the data is requested, we will collect the tweets every week from week of Feburuary 13th to the end of this project. We write a script that will automatically fetch the tweets defined by our keywords detailed below in the same time of the week (Saturday morning in our case) to minimize the bias possibly introduced by collecting tweets at different time of a week. For instance, people might feel blue at Monday while happier at Friday night. We also note that it is impractical to get data older than one week by crawl the data directly from Twitter website instead of using the API since the tweets displayed on the website are also not older than one week by our observation, which might due to the technical convinence of Twitter.

The format of data fetched from Twitter API is JSON. We use simplejson of Python to parse the data and extract the text part and time stamp of each tweet.

\subsubsection{Test Data}
We fetch our test data by querying Twitter API with predefined query tems, which are products or keywords we are interested in. For each companies covered in this study, several keywords are associated as the method we will described later. We includes top tech companies that has sufficient historical stock prices data at NASDAQ. We prefer the consumer-facing companies such as Apple for we can expect there are more discussions on the Internet and hence more meaningful information can be extract. In contrast, we will not include companies that are not mainly targeted at normal consumer, such as IBM. We also note that companies that haven't IPO or just IPO are not included, such as Twitter itself or Facebook.

For keywords, we interested in the major products of the company (such as Windows for Microsoft) or new product that are widely discussed in Twitter (such as iPhone4S for Apple). We exclude some keywords that will introduce ambiguity (such as Microsoft Office with an literarily office). We do it by querying the company at Twitter web interface, and observe the keywords that are frequenly appear with the company. For companies that don't have apprent keyword (such as Amazon) to use, we just list the company name as its keyword. In addition, to have a broad coverage and get adapt to the casual nature of Twitter, we also listed some common abbreviations for products (such as 'gmap' stands for 'googlemap'). Note that a company might be highly correalated with keywords from other companies, such as 'Amazon' and 'iTunes'. We count only the keyword directly related to the current company.

Table \ref{companies-listing} lists the tech companies we would like to study, along with their key products and hash tags that we will use for searching in the Twitter. 

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Apple &  \vbox{\hbox{\strut \#apple, \#iphone, \#iphone4s, \#iphone4,}\hbox{\strut \#siri, \#ipod, \#mac, \#macintosh}\hbox{\strut \#itunes, \#ios}} \\ \hline
        Google & \vbox{\hbox{\strut \#google, \#android, \#droid, \#chrome,}\hbox{\strut \#gmail, \#youtube, \#googlemap, \#gmap}\hbox{\strut \#googleplus, \#gplus}} \\ \hline
     Microsoft & \vbox{\hbox{\strut \#windows, \#windows8, \#xbox, \#xbox360,}\hbox{\strut \#kinect, \#msn, \#bing, \#ie}} \\ \hline
        Amazon & \#amazon, \#kindle, \#kindlefire \\ \hline
        Research in Motion & \#rim, \#blackberry \\ \hline
        Dell & \#dell \\ \hline
        Intel & \#intel, \#xeon \\ \hline
        Yahoo & \#yahoo, \#yahoomail, \#ymail, \#yim \\ \hline
        Nvidia & \#nvidia, \#tegra, \#tegra3, \#geforce \\ \hline
        Netflix & \#netflix \\
        \hline
    \end{tabular}
\caption{Companies studied in this project and the associated keywords}
\label{companies-listing}
\end{center}
\end{table}

\subsubsection{Training Data}
To train the positive, negative, we fetch tweets with positve emoticons such as ':)' and negative emoticons such as ':(', respectively. Note that we didn't use other query term to minimize the bias possibly introduced by query term. 

For objective tweets, we fetch tweets from technology news account since our major interest is tweets about technology. We list the technology news account we used here: @TechCrunch, @CNETNews, @engadget, @slashdot, @RWW, @mashable, @Gizmodo, @gigaom, @allthingsd, @TheNextWeb, @verge, @Wired, @nytimesbits, @WSJTech, @SAI, @guardiantech, @HuffPostTech.

Additionally, we will also use the JDPA \cite{JDPA}, which is an annually annotated blog entries about automobiles and cameras, to train our ngram model.

\subsection{Preprocessing}\label{preprocessing}
For data normalization, we asuume URLs and Twitter-specific features might good features while classifying tweets, we replace all urls to 'URL', hashtag to 'HASHTAG' and reply to 'REPLY'. Afterwards we do tokenization so that each token, including punctuations are splitted by space. Finally, we conduct part-of-speech tagging using NLPlib \cite{NLPlib} by Mark Watson and Jason Wiener.

\subsection{Feature Extraction}\label{feature-extraction}
Our features can be divided into four categories: count of part-of-speech tags, meta-information, count of twitter-specific syntax, unigram model trained by our training data. For count of part-of-speech tags, we will count all the possible tags produced by NLPlib, although we suppose adjective or adverbs might carry more subjective information. We might remove some of relative useless features after initial analysis.

Meta-information includes average length of sentences, average length of tokens, number of sentences, etc., which forms our second category of features. Count of first/second/third person pronouns, punctuations will also be extracted.

We also count Twitter-specific features, which includes hashtags, replies. We listed features in previous three categories in Table \ref{feature-listing}.

Finally, we extract most used unigram in our positve and negative training data, respectively. Since Pang \etal~\cite{Pang:02} reported that examing only the presence rather than counting the frequency of unigram yields better result. We will experiment whether presence of frequency produce better accuracy. In addition, we will also examine the best range of number of features we should use from the trained model.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Part-of-speech tags &  \vbox{\hbox{\strut count of: CC, DD, DT, EX, FW, IN,}\hbox{\strut JJ, JJR, JJS, LS, MD,}\hbox{\strut NN, NNS, NNP, NNPS,}\hbox{\strut PDT, POS, PRP, PRP\$,}\hbox{\strut RB, RBR, RBS, RP, SYM, TO, UH,}\hbox{\strut VB, VBD, VBG, VBN, VBP, VBZ,}\hbox{\strut WDT, WP, WP\$, WRB}} \\ \hline
           Meta-information & \vbox{\hbox{\strut count of: first/second/thrid person pronouns}\hbox{\strut count of commas, colons and semi-colons}\hbox{\strut count of: dashes, parenthses, ellipses}\hbox{\strut count of: wh-words, slang acronyms, words all in upper case}\hbox{\strut average length of sentences (in tokens)}\hbox{\strut average length of tokens (exclude punctuations tokens)}\hbox{\strut number of sentences}} \\ \hline
        Twitter-specific syntax & count of reply, hashtags \\ \hline
        Ngram models & unigram trained from training data (frequency or presence) \\ \hline
    \end{tabular}
\caption{Features used in classification}
\label{feature-listing}
\end{center}
\end{table}

\subsection{Classification}\label{classification}
For classification, we will use Naive Bayes, Support Vector Machine (SVM) and Maimum Entropy through machine learning package WEKA \cite{Witten:2005}. We will evaluate the accuracy by conducting cross-validation on training data. We will then apply the classifier to our test data. As Pang \etal~\cite{Pang:02} and Go \etal~\cite{Go:09} reported, an accuracy around 80\% for polarity classfier can be expected.

We will first classify the tweets into subjective and objective. For subjective tweets, we further classify it to positve or negative. Hence, in the end of classification stage, for each company $c_i$, we will have positive ratio $p_i$, negative ratio $n_i$ and objective ratio $b_i$, where $p_i + n_i + b_i = 1$.

\subsection{Statistical Inference}\label{statistical-inference}
Finally, we will use statstical techniques to evaluate the correlation between consumer opinions and the stock prices. We will fetch stock prices history from Yahoo Finance for the companies we studied. Stock prices is automatically fetched by a Python script to make our framework scalable. Companies studied and stock symbols are noted in Table \ref{companies-symbols}.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Apple &  AAPL \\ \hline
        Google & GOOG \\ \hline
        Microsoft & MSFT \\ \hline
        Amazon & AMZN \\ \hline
        Research in Motion & RIMM \\ \hline
        Dell & DELL \\ \hline
        Intel & INTC \\ \hline
        Yahoo & YHOO \\ \hline
        Nvidia & NVDA \\ \hline
        Netflix & NFLX \\
        \hline
    \end{tabular}
\caption{Companies studied in this project and the associated keywords}
\label{companies-symbols}
\end{center}
\end{table}

Our hypothesis is that the companies that receive more positive opinion will eventually get a positive impact on their stock prices. Hence, for a given company, we define the opinion vector as the difference of ratio between positive and negative opnion which represent the postive opnion toward a company $O = [o_i]$, where $o_i = p_i - n_i$, for $i$ from $1$ to $n$. We also have stock prices $S = [s_i]$, for $i$ from $1$ to $n$. The problem we are interested is that whether these two time series are correlated or not. We will use Pearson product-moment correlation coefficient (Pearson coefficient) to verify our hypothesis.

We define our Pearson coefficient as:
$$
PPMCC = \frac{\sum_{i=1}^{n}(O_i - \bar{O})(S_i - \bar{S})}{\sqrt{\sum_{i=1}^{n}(O_i - \bar{O})^2}\sqrt{\sum_{i=1}^{n}(S_i - \bar{S})^2}}
$$

Where $\bar{O}$ and $\bar{S}$ is the average over the opinion vector and the stock price, respectively.

Since we anticipate there might be a delaying effect from consumer evaluations to the time stock prices reflect the consumer opinion, we will use the cross correlation to find the best time window that maximize the Pearson coefficient. We will take that maximized Pearson coefficient to interpret our result.

The value of Pearson coefficient ranges from $-1$ to $1$. In variables such as stock prices that are influenced by complicated factors, a Pearson coefficient that larger than $0.5$ indicates a strong correlation, while a Pearson coefficient ranges from $0.1$ to $0.3$ can be considered as a minor correlation \cite{Cohen:1988}. We expect the Pearson coefficient of our experiment should be at least $0.1$, a lower bound of minor correlation.

Another statistical method we would like to use is the coherence of the cross-spectrum. The details will be updated later.

For the implementation of the statistical inference, Python library SciPy \cite{SciPy}, NumPy \cite{NumPy} or RPy \cite{RPy} will be used.

\section{Results}

\subsection{Data collected}
Until the time this report was written, we have collected tweets from Twitter API from February 18, 2012 to March 17, 2012, five weeks in total. We try to collect tweets at every Saturday morning unless otherwise specified. The time tweets have been collected might varies slightly, which might introduce certain degree of biases (such as people tend to feel blue at Monday morning while significantly happier at Friday night). However, we assume the biases to be minor and will be neglected by now. The number of tweets we collected for each company and keyword are summarized in Table \ref{keywords-tweet-numbers}.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | l | }
        \hline
        Apple &  \#apple & 6967 \\ \hline
        Apple &  \#iphone & 7452 \\ \hline
        Apple &  \#iphone4s & 7476 \\ \hline
        Apple &  \#iphone4 & 7485 \\ \hline
        Apple &  \#siri & 7457 \\ \hline
        Apple &  \#ipod & 7497 \\ \hline
        Apple &  \#mac & 7481 \\ \hline
        Apple &  \#macintosh & 647 \\ \hline
        Apple &  \#itunes & 7470 \\ \hline
        Apple &  \#ios & 7470 \\ \hline
        Google &  \#google & 7290 \\ \hline
        Google &  \#android & 7484 \\ \hline
        Google &  \#droid & 7446 \\ \hline
        Google &  \#googleplus & 7441 \\ \hline
        Google &  \#gplus & 742 \\ \hline
        Google &  \#gmail & 4772 \\ \hline
        Google &  \#youtube & 7444 \\ \hline
        Google &  \#chrome & 7447 \\ \hline
        Google &  \#googlemap & 143 \\ \hline
        Google &  \#gmap & 20 \\ \hline
        Microsoft &  \#windows & 7474 \\ \hline
        Microsoft &  \#windows8 & 7463 \\ \hline
        Microsoft &  \#xbox & 7492 \\ \hline
        Microsoft &  \#xbox360 & 7451 \\ \hline
        Microsoft &  \#kinect & 7480 \\ \hline
        Microsoft &  \#msn & 6752 \\ \hline
        Microsoft &  \#bing & 7057 \\ \hline
        Microsoft &  \#ie & 3577 \\ \hline
        Amazon &  \#amazon & 7442 \\ \hline
        Amazon &  \#kindle & 7424 \\ \hline
        Amazon &  \#kindlefire & 7389 \\ \hline
        Rim &  \#rim & 5989 \\ \hline
        Rim &  \#blackberry & 7487 \\ \hline
        Dell &  \#dell & 7460 \\ \hline
        Intel &  \#intel & 7452 \\ \hline
        Intel &  \#xeon & 734 \\ \hline
        Yahoo &  \#yahoo & 7429 \\ \hline
        Yahoo &  \#yahoomail & 220 \\ \hline
        Yahoo &  \#ymail & 152 \\ \hline
        Yahoo &  \#yim & 41 \\ \hline
        Nvidia &  \#nvidia & 1643 \\ \hline
        Nvidia &  \#tegra & 688 \\ \hline
        Nvidia &  \#tegra3 & 284 \\ \hline
        Nvidia &  \#geforce & 198 \\ \hline
        Netflix &  \#netflix & 7463 \\ \hline
        \hline
    \end{tabular}
\caption{Number of tweets for each keywords used in test data}
\label{keywords-tweet-numbers} % TODO: sub-table
\end{center}
\end{table}

In addition to test data, we collect 1000 tweets for each technology news site. Subjective tweets are retrieved by querying postive and negative emoticons. The number of positive and negative tweets are 6943 and 7451, respectively.

\subsection{Subjectivity Classification}
We use different classifiers to run the objectivity classification and choose the one that procues best result to apply to the test data. Accuracies of different classifiers are summarized in Table \ref{classifiers-obj}. Noted that the accuracy achieved by all the three classiers are higher than $90\%$, we suppose it's because objective tweets we retrieve from technology news have some common format and can be quite different from tweets by general users. To have objective tweets that are more similar to the test data might be one of our future work.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Naive Bayes &  91.990\% \\ \hline
        SVM & 93.738\% \\ \hline
        Decision Tree & 95.989\% \\ \hline
    \end{tabular}
\caption{Objectivity classification: accuracy achieved by different classifiers}
\label{classifiers-obj}
\end{center}
\end{table}

Since Decision Tree achieves the best accuracy, we use it to classify the objectivity of the test data. The results are summarized in Table \ref{objectivity-by-company}

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | l | l | l | }
        \hline
        Company &  \# of Obj Tweets & \# of Subj Tweets & Obj Ratio & Subj Ratio \\ \hline
        Apple &  60507 & 6895 & 0.897703332245 & 0.102296667755 \\ \hline
        Google &  44504 & 5725 & 0.886022019152 & 0.113977980848 \\ \hline
        Microsoft &  48505 & 6241 & 0.886000803712 & 0.113999196288 \\ \hline
        Amazon &  18649 & 3606 & 0.837968995731 & 0.162031004269 \\ \hline
        Research in Motion  &  11920 & 1556 & 0.884535470466 & 0.115464529534 \\ \hline
        Dell &  6759 & 701 & 0.906032171582 & 0.0939678284182 \\ \hline
        Intel &  7642 & 544 & 0.933545076961 & 0.0664549230393 \\ \hline
        Yahoo &  6573 & 1269 & 0.83817903596 & 0.16182096404 \\ \hline
        Nvidia &  2620 & 193 & 0.931389975116 & 0.0686100248845 \\ \hline
        Netflix &  6606 & 857 & 0.885166822993 & 0.114833177007 \\ \hline
    \end{tabular}
\caption{Objectivity classification results by company}
\label{objectivity-by-company}
\end{center}
\end{table}

As one can see, almost all of the raios of objectivity are around $90\%$, which is a fairly high ratio. It can also be justified by our intuition that most of people tweets for sharing information, while small portion of them are expressing their own opinion.

\subsection{Polarity Classification}
Accuracies of different classifiers are summarized in Table \ref{classifiers-polarity}. Since Decision Tree achieves the highest accuracy, we will then apply this to classify the polarity of test data.

Note that in our experiment, we discovered that Decision Tree outperformed SVM, which is different from what reported by previous works. For Decision Tree, weka.classifiers.trees.J48are used here.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Naive Bayes &  66.079\% \\ \hline
        SVM & 73.549\% \\ \hline
        Decision Tree & 76.1498\% \\ \hline
    \end{tabular}
\caption{Polarity classification: accuracy achieved by different classifiers}
\label{classifiers-polarity}
\end{center}
\end{table}

Also we noted that the accuracy we achived in similar method that test on Tweets is comparable but slightly lower than that of Go \etal \cite{Go:09}. We suppose the reasons are that we haven't tune our feature size and training size to its best.

Then, the model is used to classify tweets in testing sets. The result of polarity classfication can be summarized as Table \ref{polarity-by-company}.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | l | l | l | }
        \hline
        Company &  \# of Pos Tweets & \# of Neg Tweets & Pos Ratio & Neg Ratio \\ \hline
        Apple &  4953 & 1942 & 0.718346627991 & 0.281653372009 \\ \hline
        Google &  4470 & 1255 & 0.780786026201 & 0.219213973799 \\ \hline
        Microsoft &  4764 & 1477 & 0.76333920846 & 0.23666079154 \\ \hline
        Amazon &  2310 & 1296 & 0.640599001664 & 0.359400998336 \\ \hline
        Research in Motion &  1050 & 506 & 0.674807197943 & 0.325192802057 \\ \hline
        Dell &  529 & 172 & 0.754636233951 & 0.245363766049 \\ \hline
        Intel &  473 & 71 & 0.869485294118 & 0.130514705882 \\ \hline
        Yahoo &  1035 & 234 & 0.815602836879 & 0.184397163121 \\ \hline
        Nvidia &  144 & 49 & 0.746113989637 & 0.253886010363 \\ \hline
        Netflix &  643 & 214 & 0.750291715286 & 0.249708284714 \\ \hline
    \end{tabular}
\caption{Polarity classification results by company}
\label{polarity-by-company}
\end{center}
\end{table}

As one can see, most of the positive ratios are around $60\%$ to $80\%$. Amazon and Research in Motion have the lowest rate of positiivty. Probably because their main products Kindle and BlackBerry are relatively not positively rated by consumers. 

In contrast, Intel has the highest rate of positivity. However, we also note that the company also has the highest rate of objectivity. It might due to the fact that most of users tweet about Intel mainly for sharing information, instead of expressing their opinion toward the company or its products.

Yahoo also has one of the highest rates in positivity, which is slightly contrast to our expectation, in regards of the company's recent performance in innovation. However, if taking a look at tweets collected for the company, one will note that there is a fair amount of tweets are actually news shared from Yahoo News, rather than a Tweet that directly evaluate the company or its products. It might introduce bias to our analysis. Finding a better mechanism for collecting Tweets that directly related to evaluations of companies might also be one of our future work.

\subsection{Statistical Inference}
We collected the corresponding stock prices from Yahoo! Fianance. The following plots give the reader an overview of the relationship between ratio of sentiment analysis and its stock prices, until week five.

If we run Pearson Coeficient tests on this two vectors, the results can be summarized as Table \ref{coefficients-by-company}.

\begin{table}
\begin{center}
    \begin{tabular}{ | c || c | c | }
        \hline
        Company & Pearson Correlation Coefficient & 2-tailed p-value \\ \hline
        Apple & -0.23321991309 & 0.705769465003 \\ \hline
        Google & -0.295386442458 & 0.62944549743 \\ \hline
        Microsoft & 0.847974772848 & 0.0695100027267 \\ \hline
        Amazon & -0.485923462351 & 0.406595748044 \\ \hline
        Research in Motion & 0.883571377075 & 0.0468478552635 \\ \hline
        Dell & -0.751943424184 & 0.142659653387 \\ \hline
        Intel & 0.508100012417 & 0.382094966697 \\ \hline
        Yahoo & 0.975050083588 & 0.00471308700587 \\ \hline
        Nvidia & 0.549784427149 & 0.337059380781 \\ \hline
        Netflix & -0.362395590277 & 0.548892218988 \\ \hline
    \end{tabular}
\caption{Pearson Coefficients analysis by company}
\label{coefficients-by-company}
\end{center}
\end{table}

As one can see from the table, there are high correlated companies such as Yahoo, Microsoft or Research in Motion, however, there are also companies seemingly uncorrealted in sentiment analysis and stock prices, or even negatively correalted such as Dell or Amazon.

There might be several explanations for the result. First, our assumption is that product evaluation is crucial to the financial success to a company. However, it is not a practical assumption that the financial success is totally affected by only the competitiveness of products. Hence, product evaluation, which is a sampling toward product competitiveness will also not be the only factor that correlated with stock prices.

In addition, the Tweets we collected for study are also a sampling toward product evaluations. We collected the data basically by following some intuitive rules and we have not yet quantitatively verify the representiveness of this sampling. Besides, it is more reasonable to assume that the effect of product evaluation will take part in stock prices in the long term. However, due to technical restrictions, we have collected only a few weeks as proof-of-concept study.

Finally, we envision that there are quite a few rooms for the improvement for the accuracy of sentiment analysis. The accuracy in our study is around $70\%$ to $80\%$, which is still far from perfect.

However, we also want to note that the average of Pearson Coefficients from the study is $0.164$, which means there is an overall slight positive correalation. If one would like to incorporate our result to systems such as automatic trading system, the system may still benifited from the analysis we conducted here. In addition, we also believe that the positve correalation may improve as we find more sophiscated mechanisms to handle the limitations mentioned above.

\subsection{Further Works Until the Final Report}
In the following two weeks, we will do the following: 1) Bigram integration to increase the accuracy of sentiment analysis. 2) Test the optimal number of features in unigrams and bigrams. The reason we didn't conduct the test in this checkpoint is that WEKA encounters out of memory error when there are more than about three hundred features in an arff file. We will try to find a solution for this problem and test the optimal number of features in the following weeks. 3) Run Pearson Coefficient analysis on every possible time lag between tweets and stock prices

\section{Limitations}
Here we list some limitations. First, we assume that we can get positive tweets by querying ':)' or negative tweets by ':('. However, the semantic meaning in tweets can be complicated. Such as the following tweet by user @franki\_kuka: 'only the mail..? ;)) \#FF RT @asphodelia: \#Yahoo! Mail appears to be down...'. In this tweet, the user is probably being ironic. However, this will not be detected by our current system.

Second, collecting tweets once a week might miss some 'spike'. For instance, if an important product released at Wednesday but we happend to collect the data at Saturday morning then we might miss all the discussions on that Wednesday. Theoretically we can collect all the tweets in realtime to overcome this problem. However, we are technically limited by storage and Twitter API usage limitation.

Third, current system doesn't handle new keywords that come out in middle of our study period. For instance, if iPad3 happen to release in this period, we will not include it in the current system. One way to solve it is to propose a better automatic keyword extraction mechanism.

Fourthly, the period of collecting tweets is not long enough. Ideally we should collect for at least one or two years of tweets so that we can catch more big event such as important product releases and also reduce the impact of short-term noise.

Although products of a company is its most important and fundamental to its revenue and thus influence its stock price. However, there are also factors that can influence the price. For example, the plan that Apple decide how to use its crash on March 19, 2012 also influence its price. However, we argue that our system can also take this category of news into account, which can be one of our future work.

\section{Conclusions and Future Directions}

Including companies other than technology industry. To be effective evaluated, these companies should have relatively more tweets. Methods that calculate the number of tweets about certain company should be developed.

\bibliographystyle{abbrv}
\bibliography{reference}

\end{document}

