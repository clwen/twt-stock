\title{Correlation between Twitter Sentiment Analysis and Stock Prices}
\author{
    Chung-Lin Wen \\
    Department of Computer Science \\
    University of Toronto
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{graphicx}
\usepackage{subfigure}
\fontfamily{ptm}

\begin{document}
\maketitle

\input{defs}

\begin{abstract}
The rapid populization of social networks makes it possible to understand how consumers evaluate a company and their products directly. In this project we evaluate whether the insights collected from Twitter help us understand how stock prices might vary. We conducted sentiment analysis on Tweets collected by querying Twitter API with predefined keywords that associated with companies we are interested in. Ngram models extracted from Tweets collected by querying with emoticons along with part-of-speech tags, metadata such as sentence length are used as features for a two-phase objectivity and polarity classification. The Random Forest classifier was used for classification and accuracies that comparable to previous works are achieved. We then verified whether the result of sentiment analysis correlated with stock prices by calculating the Pearson correlation coefficients. We found overall positive correlation between these two factors. In the future, we hope to derive an automatic keyword extraction mechanism and apply the same experiment to other companies or other industries.
\end{abstract}

\section{Introduction}
The rapid populization of social networks makes it possible to understand the opinion directly from consumers, without doing additional polls or surveys. The opinion from consumer is important because it reflects how they evaluate the product. It is very likely they will shop from the same company only if they have an overall positive experience about the products/services they paid for. Otherwise, they might turn to other alternatives. Additionally, public opinion will also influence the shopping behavior of their friends, family, collegues, whoever their connection is, which is especially true on the social networks. All of these buying decisions will eventually generate revenue or incur losses for the companies, depending on whether their consumers are happy or not. Hence, it is a reasonable assumption that the performance of a company can be predicted by consumer opinions to some extent, which is especially true if we have sufficient amount of data. 

Consequently, we propose to verify that the performance of a company can be predicted from opinions collected from social networks. There are several alternatives for measuring the success of a company, such as net cash flow, turnover growth, gross margin, etc. However, most of them are biased by size, industry, subfield they are working on. In contrast, stock price doesn't suffer from these biases. In addition, almost all the metrics mentioned before will eventually reflected on the stock price.

To verify our assumption, we start with a few companies that are widely discussed over the web. Besides, ideally, its revenue should primarily depend on a few key products, which means we can use a few keywords to collect the data we need. For instance, \#iphone4s, \#iphone4, \#iphone3, \#ipad, \#macbookair, etc. Apple Incorporation, for instance, is one of the companies we start with, since it meets the condition we discussed above. It is widely discussed on social network. If one query 'Apple' or 'iPhone' on Twitter, most of the tweets are published within several minutes, indicates the frequency of tweets mentioning it is fairly high. In contrast, if one query 'Baidu' on Twitter, there are tweets older than one hour even in the first page.

For the social network platform we will use, we favor Twitter as our major data source for the following reasons. First, the amount of data is huge enough for our usage. Second, its microblogging nature provide sentence level texts that focus on usually a single idea (e.g., I love \#iPhone4S coz I had a lot of fun playing with \#Siri), thus reducing the noise we have to deal with.

After collecting the data, we will classified the tweets into positive, neutral (or irrelevant), negative as the measurements of consumer evaluation. After that, we will study the relationship between this evaluation and the stock price of corresponding companies using statistical techniques such as Pearson correlation coefficients.

\section{Related Works}
For its wide and valuable usage, sentiment analysis, which is to classify the polarity (positive, negative) given a document or a sentence, has been widely studied yet still an extremely challenging problem. Pang and Lee \cite{Pang:08} did a comprehensive survey about the state-of-the-art techniques in the sentimental area. Liu \cite{Liu:10} summarizes the theory and techniques of the subfield for the purpose of teaching and learning.

Turney \etal~\cite{Turney:02} pioneered in the field by firstly address the problem. He first extracted phrases that might contain subjective evaluation using its POS tags, then applied PMI-IR algorithm \cite{Turney:01} to determine the semantic orientation by measuring the similarity of the extracted phrases and "excellent", "poor" (regarded as positive and negative, respectively, in this paper). Finally, the polarity of a document was given by averaging the semantic orientations of all the extracted phrases.

In contrast, rather than using predefined bags of words for polarity, Pang \etal~\cite{Pang:02} approached the problem by using features that are prior-knowledge-free. They used unigrams and bigrams extracted from corpus according to the frequencies then applied the standard machine learning classifiers, which includes Naive Bayes, Maximum Entropy and Support Vector Machines (SVM). They concludes that using the presence of unigrams and using SVM as classifier yields the best accuracy (82.9\%).

Besides polarity, detection of opinion strength is also made possible by further research. Thelwall \etal~\cite{Thelwall:10} proposed to use a dictionary of sentiment words with associated strength measures as one of its feature to detect the opinion strength. Repeated letters, punctuation, emoticons were also used to make the system suitable for informal text.

Since the popularity of social networks, especially the microblogging has grown quickly in the recent years, there are several systems utilized these platforms as the corpus of sentiment analysis. Go \etal~\cite{Go:09} first experimented sentiment analysis on Twitter, the most popular microblogging platform. They basically followed the algorithm used as Pang \etal~\cite{Pang:02}. Unigrams, bigrams, POS tags are used as features; Bayes, SVM, Maximum Entropy were used as classifiers. They report that the combination of unigram and Bayes with mutual information feature selection method yields the best result, which is slightly different from Pang \etal~\cite{Pang:02}.

Pak and Paroubek \cite{Pak:10} conductged a two-phase classification to improve the accuracy. Tweets were first divided to objective and subjective ones, then subjective tweets were further divided to positive and negative ones. For training data, they used pre-defined emoticons to get the subjective data, while retrieved Tweets from popular newspaper accounts as objective data. In terms of features and classifiers, they followed a similar setup as Pang \etal~\cite{Pang:02} but also included trigrams as one of the features. They conclude that bigram yields the best result since it maintains a good balance between coverage and also able to catch sentiment expressions such as negation.

Barbosa and Feng \cite{Barbosa:10} propose new features to increase the accuracy of Twitter sentiment analysis. They investigate the use of two additional sets of features. One of the sets is the meta features. In addition to POS tags, a word list with pre-defined polarity and subjectivity is used. Note that if the word is preceded by negative expression (e.g., not, never), the polarity will be reversed. The other set they propose is the use of tweets syntax feature, such as retweet, hashtags, reply, link.

Agarwal \etal~\cite{Agarwal:11} claim that they achieved a better result by incorporating the tree kernel model, which is a tree of features such as stop words, target, URL, English words. They compared the tree kernel based model with unigram model and feature-based model from previous work and conclude that the tree kernel model outperforms the other two. Besides, they claimed that using random sampled streaming tweets instead of that searched by keywords introduces fewer biases.

There are different applications on data mining of Twitter or other social networks. Petrovic \etal~\cite{Petrovic:10} proposed a system to detect new event on Twitter, on which news propagate even quicker than the traditional news medias.

There are also applications for sentiment analysis, although not necessarily using Twitter data as a corpus. Besides only indicating the review is positive or negative, Kim \etal~\cite{Kim:06} proposed a system that also extract the reasons behind these positive and negative opinions by an maximum entropy model.

Blair-Goldensohn \etal~\cite{Blair:08} propose to incorporate the readily available information other than text itself to produce a better result. They have also integrated a summary of customer review, which includes different aspects (e.g., food, decor, service, etc in a restaurant) and the corresponding reasons customers love or hates a specific store or product.

Among all the applications, we are most interested about applications that utilize sentiment analysis or opinion mining to predict some metrics in the real world. Asur and Huberman \cite{Asur:10} used Twitter data to predict the box-office revenues for movies. They study 24 movies released between November 2009 and February 2010 and conclude that the average tweet-rates (published tweets per hour) is statiscally correalated with its box-office revenue. They also report that sentiment content can improve the correalation although not significant as tweet-rates themselves.

There are also financial applications that used sentiment analysis for predicting stock prices. Schumaker \etal~\cite{Schumaker:2009} propose a system, AZFinText, which can predict the stock price of a company that mentioned by a financial news. They assume financial news will have impact on the decision made within stock buyers, and it will have immediate effect so that the price after 20 minutes the news is released can be predicted. They implemented the SVR Sequential Minimal Optimization function through WEKA \cite{Witten:2005}, while using proper nouns and results of sentiment analysis as features. They conclude that both subjectivety and polarity contribute to the accuracy of stock prices prediction.

There are also private companies such as Topsy Labs \cite{Topsy} doing the same investigation as we do. However, they didn't publish the algorithms and methodology they used.

\section{Experiment Setup}
\subsection{Overview}
In this section, we describe how we set up our experiment. Our experiment contains mainly two parts: sentiment analysis for Tweets and statistical inference of relationship between sentiment analysis and stock prices. We first conducted a two-phase (subjectivity and then polarity) sentiment analysis then we investigated whether the ratio of sentiment analysis are positive correalated to stock prices or not by using statistical methods. For sentiment analysis, it can be further decomposed into the following modules: data collection, preprocessing, feature extraction and classification. 

We detail each module by a subsection in the following of this section. In subsection \ref{data-collection} we discuss how our training data and test data are defined and collecetd. In subsection \ref{preprocessing} we discuss what kind of preprocessing we will apply to our data. Feature definition and selection is discussed in subsection \ref{feature-extraction}. Then we detail the classification process in subsection \ref{classification}. Finally, we discuss the statiscal method to be used for relationship between sentiment analysis and stock prices in subsection \ref{statistical-inference}.

\subsection{Data Collection}\label{data-collection}
Our data can be divided into the following portions: test data, which contain tweets about products or companies we are interested; training data, which can be further divide to positive training data, negative training data, and objective training data. We first discuss the methodolgy how the data were collected, then we define our test data and training data subsequently.

\subsubsection{Methodology}
For the reasons stated before, we mainly used Twitter as our primary corpus. Ideally, we would like to collect Tweets back to one or two years to catch the trends that whenever a popular came to the market, the stock prices soared (such as iPhone4), or reversely, whenever a product came to the market but not fully satisfied consumer's expection, the stock price plumped (such as iPhone4S). 

However, since Twitter API only allows external users to parse the Tweets no longer than a week from the time the data is requested, we collected the Tweets every week from week of Feburuary 13th to the end of this project, which was seven weeks to the time this report was written. We wrote a script that automatically fetches the Tweets defined by our keywords detailed below in the same time of the week (Saturday morning in our case) to minimize the bias possibly introduced by collecting tweets at different time of a week. For instance, people might feel bluer at Monday mornings while happier at Friday nights. We also note that it is impractical to get data older than one week by crawling the data directly from the Twitter website instead of using the API since the Tweets displayed on the website are also not older than one week according to our observation, which might due to the technical convinence of Twitter.

The format of data fetched from Twitter API was JSON. We used simplejson of Python to parse the data and extracted the text part and time stamp of each tweet.

\subsubsection{Test Data}
We fetched our test data by querying Twitter API with predefined query tems, which are products or keywords we are interested in. For each companies covered in this study, several keywords were associated as the method we described later. We included top tech companies that has sufficient historical stock prices data at NASDAQ. We prefer the consumer-facing companies such as Apple for we can expect there are more discussions on the Internet and hence more meaningful information can be extract. Besides, the stock prices of this kind of companies are also more likey to be influenced by consumer evaluation. In contrast, we will not include companies that are not mainly targeted at normal consumer, such as IBM. We also note that companies that haven't IPO or just IPO are not included, such as Twitter itself or Facebook.

For keywords, we interested in the major products of the company (such as Windows for Microsoft) or new product that are widely discussed in Twitter (such as iPhone4S for Apple). We excluded some keywords that might introduce ambiguity (such as '\#office' for Microsoft Office that might be confused with an office in its literary meaning). We do it by querying the company at Twitter web interface, and observe the keywords that are frequenly appear with the company. For companies that don't have apprent keyword (such as Amazon) to use, we simply listed the company name as its keyword. In addition, in order to have a broader coverage and get adapt to the casual nature of Twitter, we also listed some common abbreviations for products (such as 'gmap' stands for 'googlemap'). Note that a company might be highly correalated with keywords from other companies, such as 'Amazon' and 'iTunes'. We counted only the keywords directly related to the current company.

Table \ref{companies-listing} lists the tech companies we studied in the project, along with their key products and hash tags that we used for searching in the Twitter. 

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Apple &  \vbox{\hbox{\strut \#apple, \#iphone, \#iphone4s, \#iphone4,}\hbox{\strut \#siri, \#ipod, \#mac, \#macintosh}\hbox{\strut \#itunes, \#ios}} \\ \hline
        Google & \vbox{\hbox{\strut \#google, \#android, \#droid, \#chrome,}\hbox{\strut \#gmail, \#youtube, \#googlemap, \#gmap}\hbox{\strut \#googleplus, \#gplus}} \\ \hline
     Microsoft & \vbox{\hbox{\strut \#windows, \#windows8, \#xbox, \#xbox360,}\hbox{\strut \#kinect, \#msn, \#bing, \#ie}} \\ \hline
        Amazon & \#amazon, \#kindle, \#kindlefire \\ \hline
        Research in Motion & \#rim, \#blackberry \\ \hline
        Dell & \#dell \\ \hline
        Intel & \#intel, \#xeon \\ \hline
        Yahoo & \#yahoo, \#yahoomail, \#ymail, \#yim \\ \hline
        Nvidia & \#nvidia, \#tegra, \#tegra3, \#geforce \\ \hline
        Netflix & \#netflix \\
        \hline
    \end{tabular}
\caption{Companies studied in this project and the associated keywords}
\label{companies-listing}
\end{center}
\end{table}

\subsubsection{Training Data}
To train the positive, negative, we fetched Tweets with postive emoticons such as ':)' and negative emoticons such as ':(', respectively. Note that we didn't use other query terms so that we can minimize the bias possibly introduced by query term. 

For objective tweets, we fetched tweets from technology news account since our current interest is in Tweets about technology. We list the technology news accounts we used here: @TechCrunch, @CNETNews, @engadget, @slashdot, @RWW, @mashable, @Gizmodo, @gigaom, @allthingsd, @TheNextWeb, @verge, @Wired, @nytimesbits, @WSJTech, @SAI, @guardiantech, @HuffPostTech.

\subsection{Preprocessing}\label{preprocessing}
For data normalization, we assumed that URLs and Twitter-specific features might be good features while classifying tweets, we replaced all urls to 'URL', hashtags to 'HASHTAG' and replies to 'REPLY'. Afterwards we conducted tokenization so that each token, including punctuations are splitted by spaces. Finally, we conducted part-of-speech tagging using NLPlib \cite{NLPlib} by Mark Watson and Jason Wiener.

\subsection{Feature Extraction}\label{feature-extraction}
Our features can be divided into four categories: count of part-of-speech tags, meta-information, count of Twitter-specific syntax, ngram model trained by our training data. For count of part-of-speech tags, we counted all the possible tags produced by NLPlib, although we suppose adjective or adverbs might carry more subjective information.

Meta-information includes average length of sentences, average length of tokens, number of sentences, etc., formed our second category of features. Count of first/second/third person pronouns, punctuations were also be extracted.

We also counted the Twitter-specific features, which includes hashtags, replies. We listed features in previous three categories in Table \ref{feature-listing}.

Finally, we extracted most used unigram in our positve and negative training data, respectively. In addition, we will also examine the best range of number of features we should use from the trained model.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Part-of-speech tags &  \vbox{\hbox{\strut count of: CC, DD, DT, EX, FW, IN,}\hbox{\strut JJ, JJR, JJS, LS, MD,}\hbox{\strut NN, NNS, NNP, NNPS,}\hbox{\strut PDT, POS, PRP, PRP\$,}\hbox{\strut RB, RBR, RBS, RP, SYM, TO, UH,}\hbox{\strut VB, VBD, VBG, VBN, VBP, VBZ,}\hbox{\strut WDT, WP, WP\$, WRB}} \\ \hline
           Meta-information & \vbox{\hbox{\strut count of: first/second/thrid person pronouns}\hbox{\strut count of commas, colons and semi-colons}\hbox{\strut count of: dashes, parenthses, ellipses}\hbox{\strut count of: wh-words, slang acronyms, words all in upper case}\hbox{\strut average length of sentences (in tokens)}\hbox{\strut average length of tokens (exclude punctuations tokens)}\hbox{\strut number of sentences}} \\ \hline
        Twitter-specific syntax & count of reply, hashtags \\ \hline
        Ngram models & unigram trained from training data (in frequency) \\ \hline
    \end{tabular}
\caption{Features used in classification}
\label{feature-listing}
\end{center}
\end{table}

\subsection{Classification}\label{classification}
For classification, we used Naive Bayes, Support Vector Machine (SVM), Decision Tree, and Random Forest through machine learning package WEKA \cite{Witten:2005}. We evaluated the accuracy by conducting cross-validation on training data. We then applied the classifier to our test data. As Pang \etal~\cite{Pang:02} and Go \etal~\cite{Go:09} reported, an accuracy around 80\% for polarity classifier can be expected.

We first classified the tweets into subjective and objective ones. For subjective tweets, we further classified it to positve or negative ones. Hence, in the end of classification stage, for each company $c_i$, we will have positive ratio $p_i$, negative ratio $n_i$ and objective ratio $b_i$, where $p_i + n_i + b_i = 1$.

\subsection{Statistical Inference}\label{statistical-inference}
Finally, we used statstical techniques to evaluate the correlation between consumer opinions and the stock prices. We fetched stock prices history from Yahoo! Finance for the companies we studied. Stock prices was automatically fetched by a Python script to make our framework scalable. Companies studied and stock symbols are noted in Table \ref{companies-symbols}.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Apple &  AAPL \\ \hline
        Google & GOOG \\ \hline
        Microsoft & MSFT \\ \hline
        Amazon & AMZN \\ \hline
        Research in Motion & RIMM \\ \hline
        Dell & DELL \\ \hline
        Intel & INTC \\ \hline
        Yahoo & YHOO \\ \hline
        Nvidia & NVDA \\ \hline
        Netflix & NFLX \\
        \hline
    \end{tabular}
\caption{Companies studied in this project and the associated keywords}
\label{companies-symbols}
\end{center}
\end{table}

Our hypothesis is that the companies that receive more positive opinion will eventually get a positive impact on their stock prices. Hence, for a given company, we define the opinion vector as the ratio positive opinion toward a company $O = [p_i]$, for $i$ from $1$ to $n$. We also have stock prices $S = [s_i]$, for $i$ from $1$ to $n$. The problem we are interested is that whether these two time series are correlated or not. We used Pearson product-moment correlation coefficient (Pearson coefficient) to verify our hypothesis.

We define our Pearson coefficient as:
$$
PPMCC = \frac{\sum_{i=1}^{n}(O_i - \bar{O})(S_i - \bar{S})}{\sqrt{\sum_{i=1}^{n}(O_i - \bar{O})^2}\sqrt{\sum_{i=1}^{n}(S_i - \bar{S})^2}}
$$

Where $\bar{O}$ and $\bar{S}$ is the average over the opinion vector and the stock price, respectively.

% Since we anticipate there might be a delaying effect from consumer evaluations to the time stock prices reflect the consumer opinion, we will use the cross correlation to find the best time window that maximize the Pearson coefficient. We will take that maximized Pearson coefficient to interpret our result.

The value of Pearson coefficient ranges from $-1$ to $1$. In variables such as stock prices that are influenced by complicated factors, a Pearson coefficient that larger than $0.5$ indicates a strong correlation, while a Pearson coefficient ranges from $0.1$ to $0.3$ can be considered as a minor correlation \cite{Cohen:1988}. We expect the Pearson coefficient of our experiment should be at least $0.1$, a lower bound of minor correlation.

For the implementation of the statistical inference, Python library SciPy \cite{SciPy}, NumPy \cite{NumPy} or RPy \cite{RPy} were used.

\section{Results}

\subsection{Data collected}
Until the time this report was written, we have collected tweets from Twitter API from February 18, 2012 to March 31, 2012, seven weeks in total. We collected tweets at every Saturday morning unless otherwise specified. The time Tweets had been collected might varies slightly, which might introduce certain degree of biases (such as people tend to feel bluer at Monday mornings while significantly happier at Friday nights). However, we assume that the biases should be minor and can be temporarily neglected by now. The number of Tweets we collected for each company and keyword are summarized in Table \ref{keywords-tweet-numbers} and \ref{keywords-tweet-numbers-02}.

We have also tried to verify Zipf's law on the test data. The ranks and freqencies are summarized as Figure \ref{rank-freq}. By the figure we observe that the test data seems not to follow the Zipf's law very well. It might due to the fact that Tweets are at most 140 chars each, hence users tend to use similar words. Also, due to the fact that Tweets have some specific features such as reply, retweet, or the inclusion of link, the figure we produced has a fairly long right tail. We cut it off around rank $2000$.

\begin{figure}
\centering
    \includegraphics[scale=0.65]{figs/zipf.eps}
\caption{Verification of Zipf's law on our test data. Ranks and Frequencies are plotted}
\label{rank-freq}
\end{figure}

\begin{table}
\begin{center}
    \begin{tabular}{ | c || l | c | }
        \hline
        \textbf{Company} &  \textbf{Keyword} & \textbf{\# of Tweets} \\ \hline
        \hline
        Apple &  \#apple & 9957 \\ \hline
        Apple &  \#iphone & 10435 \\ \hline
        Apple &  \#iphone4s & 10461 \\ \hline
        Apple &  \#iphone4 & 10475 \\ \hline
        Apple &  \#siri & 10438 \\ \hline
        Apple &  \#ipod & 10487 \\ \hline
        Apple &  \#mac & 10473 \\ \hline
        Apple &  \#macintosh & 806 \\ \hline
        Apple &  \#itunes & 10449 \\ \hline
        Apple &  \#ios & 10466 \\ \hline
        Google &  \#google & 10148 \\ \hline
        Google &  \#android & 10471 \\ \hline
        Google &  \#droid & 10400 \\ \hline
        Google &  \#googleplus & 10426 \\ \hline
        Google &  \#gplus & 911 \\ \hline
        Google &  \#gmail & 6777 \\ \hline
        Google &  \#youtube & 10409 \\ \hline
        Google &  \#chrome & 10437 \\ \hline
        Google &  \#googlemap & 232 \\ \hline
        Google &  \#gmap & 31 \\ \hline
        Microsoft &  \#windows & 10470 \\ \hline
        Microsoft &  \#windows8 & 10459 \\ \hline
        Microsoft &  \#xbox & 10431 \\ \hline
        Microsoft &  \#xbox360 & 10434 \\ \hline
        Microsoft &  \#kinect & 10468 \\ \hline
        Microsoft &  \#msn & 9074 \\ \hline
        Microsoft &  \#bing & 9126 \\ \hline
        Microsoft &  \#ie & 5118 \\ \hline
        Amazon &  \#amazon & 10405 \\ \hline
        Amazon &  \#kindle & 10402 \\ \hline
        Amazon &  \#kindlefire & 10144 \\ \hline
        \hline
    \end{tabular}
\caption{Number of tweets for each keywords used in test data}
\label{keywords-tweet-numbers} % TODO: sub-table
\end{center}
\end{table}

\begin{table}
\begin{center}
    \begin{tabular}{ | c || l | c | }
        \hline
        \textbf{Company} &  \textbf{Keyword} & \textbf{\# of Tweets} \\ \hline
        \hline
        Research in Motion &  \#rim & 8673 \\ \hline
        Research in Motion &  \#blackberry & 10482 \\ \hline
        Dell &  \#dell & 10429 \\ \hline
        Intel &  \#intel & 9846 \\ \hline
        Intel &  \#xeon & 788 \\ \hline
        Yahoo &  \#yahoo & 10399 \\ \hline
        Yahoo &  \#yahoomail & 384 \\ \hline
        Yahoo &  \#ymail & 176 \\ \hline
        Yahoo &  \#yim & 53 \\ \hline
        Nvidia &  \#nvidia & 2338 \\ \hline
        Nvidia &  \#tegra & 977 \\ \hline
        Nvidia &  \#tegra3 & 347 \\ \hline
        Nvidia &  \#geforce & 324 \\ \hline
        Netflix &  \#netflix & 10452 \\ \hline
        \hline
    \end{tabular}
\caption{Number of tweets for each keywords used in test data (Con't)}
\label{keywords-tweet-numbers-02}
\end{center}
\end{table}

In addition to test data, we collected $1400$ tweets for each technology news site choosed. Subjective tweets are retrieved by querying positive and negative emoticons such as ':)' or ':('. The number of positive and negative Tweets are $9886$ and $10431$, respectively.

\subsection{Subjectivity Classification}
We used different classifiers to run the objectivity classification and choose the one that produces best result that was applied to the test data. Accuracies of different classifiers are summarized in Table \ref{classifiers-obj}. We observed that the accuracy achieved by all the four classiers are higher than $90\%$. It might due to the fact that objective Tweets retrieved from technology news have some common format and can be quite distinguishable from Tweets by general users. To have objective Tweets that are more representative to our test data should be one of the future works.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        \textbf{Classfier} &  \textbf{Accuracy} \\ \hline
        \hline
        Naive Bayes &  91.990\% \\ \hline
        SVM & 93.738\% \\ \hline
        Decision Tree & 95.989\% \\ \hline
        Random Forest & 96.927\% \\ \hline
    \end{tabular}
\caption{Objectivity classification: accuracy achieved by different classifiers}
\label{classifiers-obj}
\end{center}
\end{table}

Since the Random Forest achieved the best accuracy, we used it to classify the objectivity of the test data. The results are summarized in Table \ref{objectivity-by-company}

\begin{table}
\begin{center}
    \begin{tabular}{ | c || c | c | c | c | }
        \hline
        \textbf{Company} &  \textbf{Obj Twts} & \textbf{Subj Twts} & \textbf{Obj Ratio} & \textbf{Subj Ratio} \\ \hline
        \hline
        Apple &  63479 & 30968 & 0.672112401664 & 0.327887598336 \\ \hline
        Google &  50798 & 19444 & 0.723185558498 & 0.276814441502 \\ \hline
        Microsoft &  58170 & 17410 & 0.769648055041 & 0.230351944959 \\ \hline
        Amazon &  24379 & 6572 & 0.787664372718 & 0.212335627282 \\ \hline
        Research in Motion &  13195 & 5960 & 0.688854085095 & 0.311145914905 \\ \hline
        Dell &  8344 & 2085 & 0.800076709176 & 0.199923290824 \\ \hline
        Intel &  8884 & 1750 & 0.83543351514 & 0.16456648486 \\ \hline
        Yahoo &  8705 & 2307 & 0.79050127134 & 0.20949872866 \\ \hline
        Nvidia &  3274 & 712 & 0.821374811841 & 0.178625188159 \\ \hline
        Netflix &  5923 & 4529 & 0.56668580176 & 0.43331419824 \\ \hline
    \end{tabular}
\caption{Objectivity classification results by company}
\label{objectivity-by-company}
\end{center}
\end{table}

As one can see, almost all of the ratios of objectivity are above $70\%$, which is a fairly high ratio. It can also be justified by our intuition that most of people tweets for sharing information, while small portion of them are expressing their own opinion.

\subsection{Polarity Classification}
Accuracies of different classifiers are summarized in Table \ref{classifiers-polarity}. Since the Random Forest achieved the highest accuracy, we then applied the classifier to predict the polarity of test data.

Note that in our experiment, we discovered that the Random Forest outperformed SVM, which is different from what reported by previous works. In implementation, $weka.classifiers.trees.RandomForest$ were used for the Random Forest.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        \textbf{Classfier} &  \textbf{Accuracy} \\ \hline
        \hline
        Naive Bayes &  65.5642\% \\ \hline
        SVM & 71.9609\% \\ \hline
        Decision Tree & 76.3243\% \\ \hline
        Random Forest & 79.1518\% \\ \hline
    \end{tabular}
\caption{Polarity classification: accuracy achieved by different classifiers}
\label{classifiers-polarity}
\end{center}
\end{table}

Also we noted that the accuracy we achived in similar method that test on Tweets is comparable but slightly lower than that of Go \etal~\cite{Go:09}. We suppose the reasons are that we haven't tune the feature size and training size to its best.

Then, the model was used to classify tweets in testing sets. The result of polarity classfication can be summarized as Table \ref{polarity-by-company}.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | l | l | l | }
        \hline
        \textbf{Company} &  \textbf{Pos Tweets} & \textbf{Neg Tweets} & \textbf{Pos Ratio} & \textbf{Neg Ratio} \\ \hline
        \hline
        Apple &  29286 & 1682 & 0.945685869284 & 0.0543141307156 \\ \hline
        Google &  16892 & 2552 & 0.868751285744 & 0.131248714256 \\ \hline
        Microsoft &  15851 & 1559 & 0.910453762206 & 0.0895462377944 \\ \hline
        Amazon &  5887 & 685 & 0.895769933049 & 0.104230066951 \\ \hline
        Rim &  5609 & 351 & 0.94110738255 & 0.0588926174497 \\ \hline
        Dell &  1839 & 246 & 0.882014388489 & 0.117985611511 \\ \hline
        Intel &  1627 & 123 & 0.929714285714 & 0.0702857142857 \\ \hline
        Yahoo &  2036 & 271 & 0.882531426094 & 0.117468573906 \\ \hline
        Nvidia &  621 & 91 & 0.872191011236 & 0.127808988764 \\ \hline
        Netflix &  4217 & 312 & 0.931110620446 & 0.068889379554 \\ \hline
    \end{tabular}
\caption{Polarity classification results by company}
\label{polarity-by-company}
\end{center}
\end{table}

As one can see, most of the positive ratios are around $80\%$ to $95\%$. Dell and Yahoo have the lowest rate of positive evaluation. Probably because their recent major products are relatively not positively rated by consumers. Contrary to our expectation, Google also has one of the lowest rate of positive evaluation.

Apple has one of the highest rate of positive evaluation, which is align with our expectation, since the recent major products was welcomed by the market, generally speaking. However, in the same industry, Research in Motion also get one of the highest rate of positive evaluation, which is also contrary to our expectation too.

Generally speaking, we regard the results in week five is better than week seven, for the positive ratio is generally higher than we expected. Besides, the corresponding positive ratios are also sometimes not aligned with our intuition. We discuss the possible reasons later in this section.

% Yahoo also has one of the highest rates in positivity, which is slightly contrast to our expectation, in regards of the company's recent performance in product innovation. However, if one takes a closer look at Tweets collected for the company, an observation can be made that there is a fair amount of Tweets are actually news shared from the Yahoo! News, rather than a Tweet that directly evaluates the company or its products. It might introduce bias to our analysis. Finding a better mechanism for collecting Tweets that directly related to evaluations of companies might also be one of our future works.

\subsubsection{Using Bigrams as Features}

We have also tried to include bigrams as featuers. We included 123 unique bigram features that are top frequencies in positive or negative Tweets, excluding emoticons as features. We combine the bigram features as previous used unigram and POS tags, etc. and get the results as Table \ref{bigrams-polarity} shows.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        \textbf{Classfier} &  \textbf{Accuracy} \\ \hline
        \hline
        Naive Bayes &  65.7736\% \\ \hline
        SVM & 73.1303\% \\ \hline
        Decision Tree & 76.333\% \\ \hline
    \end{tabular}
\caption{Polarity classification that includes bigram features}
\label{bigrams-polarity}
\end{center}
\end{table}

As we can see, the inclusion of bigram will only produce a slight improvement toward in terms of accuracy in all the three classifiers tested: Naive Bayes, SVM, and Decision Tree. The observation also align with that of Pang \etal~\cite{Pang:02}. In order to avoid overfitting and to maintain a simpler structure of features, we will not use bigrams in the following procedures.

Theoritically, we can examine the effect of bigram features more systematically in a way similar to test optimal feature size, which is described in the following. That is, using different number of bigrams and record the differences of with and without the bigram in combination with different sizes of unigram. However, the execution time of each test sometimes cost hours when feature size gets bigger. Hence, we leave it as one of the future works.

\subsection{Optimal Feature Size}

We have also tested the polarity classification under different size of unigram features for four classifiers we used. The accuracies achieved can be summarized as Figure \ref{classifiers-feature-size} and Table \ref{classifiers-feature-size-tbl}. Several conclusions can be drawn: First, the overall performance of Random Forest are better than that of the other classifiers. Second, the performance of Random Forest peaked at feature size around $1000$. Hence, we choose this as our feature size in the polarity classification. Specifically speaking, it was $1043$, which extract $800$ features from positive, negative training set respectively and remove the overlapping ones.

\begin{figure}
\centering
    \subfigure[]{\includegraphics[scale=0.4]{figs/bayes.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/svm.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/tree.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/forest.eps}}
\caption{The accuracies achieved by different classfiers in different feature size}
\label{classifiers-feature-size}
\end{figure}

\begin{table}
\begin{center}
    \begin{tabular}{ | c || c | c | c | c | }
        \hline
        \textbf{Feature Size} & \textbf{Naive Bayes} & \textbf{SVM} & \textbf{Decision Tree} & \textbf{Random Forest} \\ \hline
        \hline
        117 & 65.2937\% & 71.7253\% & 75.9665\% & 78.8376\% \\ \hline
        244 & 65.6776\% & 72.7463\% & 76.3941\% & 79.1518\% \\ \hline
        370 & 65.7474\% & 73.235\% & 76.7344\% & 79.3612\% \\ \hline
        493 & 65.8085\% & 74.3171\% & 76.7432\% & 79.5619\% \\ \hline
        627 & 65.826\% & 74.6226\% & 76.7868\% & 79.623\% \\ \hline
        760 & 65.9656\% & 74.8931\% & 76.7344\% & 79.2041\% \\ \hline
        891 & 65.9918\% & 75.0676\% & 76.6385\% & 79.2739\% \\ \hline
        1043 & 65.9918\% & 75.6087\% & 76.7083\% & 79.6492\% \\ \hline
        1182 & 66.018\% & 75.7396\% & 76.7083\% & 78.5845\% \\ \hline
        1331 & 66.0267\% & 75.6785\% & 76.717\% & 78.8725\% \\ \hline
        1467 & 66.0267\% & 75.8007\% & 76.4552\% & 79.6405\% \\ \hline
        1609 & 66.0354\% & 76.1323\% & 76.4901\% & 79.431\% \\ \hline
        1762 & 66.0267\% & 75.9141\% & 76.621\% & 79.0121\% \\ \hline
        1930 & 66.0354\% & 76.2894\% & 76.6035\% & 79.3176\% \\ \hline
        2084 & 66.0442\% & 76.6734\% & 76.6297\% & 79.3961\% \\ \hline
        2240 & 66.0616\% & 76.6123\% & 76.6559\% & 79.3612\% \\ \hline
        2400 & 66.0616\% & 76.8392\% & 76.6385\% & 79.1692\% \\ \hline
        2543 & 66.0616\% & 77.0486\% & 76.6385\% & 79.1518\% \\ \hline
        2693 & 66.0616\% & 77.0224\% & 76.6559\% & 79.527\% \\ \hline
        2848 & 66.0616\% & 77.0486\% & 76.6734\% & 79.3088\% \\ \hline
    \end{tabular}
\caption{The accuracies achieved by different classfiers in different feature size}
\label{classifiers-feature-size-tbl}
\end{center}
\end{table}

\subsection{Statistical Inference}
We collected the corresponding stock prices from the Yahoo! Finance. Figure \ref{xyplots-ratio-price} and Figure \ref{xyplots-ratio-price-02} give the reader an insight of the relationship between ratio of sentiment analysis and its stock prices, until week seven. The red lines indicate the linear regression model generated by $R$. We also summarized the standard residue errors in Table \ref{residue-errors}. Note that companies that have a higher stock prices will tend to have a higher standard residue error.

\begin{figure}
\centering
    \subfigure[]{\includegraphics[scale=0.4]{figs/apple_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/google_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/microsoft_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/amazon_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/rim_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/dell_plot.eps}}
\caption{Scatter plots for ratios of positive Tweets and stock prices}
\label{xyplots-ratio-price}
\end{figure}

\begin{figure}
\centering
    \subfigure[]{\includegraphics[scale=0.4]{figs/intel_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/yahoo_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/nvidia_plot.eps}}
    \subfigure[]{\includegraphics[scale=0.4]{figs/netflix_plot.eps}}
\caption{Scatter plots for ratios of positive Tweets and stock prices (Con't)}
\label{xyplots-ratio-price-02}
\end{figure}

\begin{table}
\begin{center}
    \begin{tabular}{ | c || c | }
        \hline
        \textbf{Company} & \textbf{Standard Residue Error} \\ \hline
        \hline
        Amazon & 7.838 \\ \hline
        Apple & 34.18 \\ \hline
        Dell & 0.6238 \\ \hline
        Google & 13.8 \\ \hline
        Intel & 0.5802 \\ \hline
        Microsoft & 0.329 \\ \hline
        Netflix & 5.311 \\ \hline
        Nvidia & 0.5766 \\ \hline
        Research in Motion & 0.5948 \\ \hline
        Yahoo & 0.2969 \\ \hline
    \end{tabular}
\caption{Standard residue errors}
\label{residue-errors}
\end{center}
\end{table}

If we run Pearson Coeficient tests on this two vectors, the results can be summarized as Table \ref{coefficients-by-company}.

\begin{table}
\begin{center}
    \begin{tabular}{ | c || c | c | }
        \hline
        \textbf{Company} & \textbf{Pearson Correlation Coefficient} & \textbf{2-tailed p-value} \\ \hline
        \hline
        Apple & 0.569057264858 & 0.182433613581 \\ \hline
        Google & -0.533343926118 & 0.217646661565 \\ \hline
        Microsoft & -0.753330831187 & 0.0505590882597 \\ \hline
        Amazon & 0.573260924741 & 0.178486767023 \\ \hline
        Research in Motion & 0.405919970286 & 0.366229509373 \\ \hline
        Dell & -0.0821672150498 & 0.86097901078 \\ \hline
        Intel & 0.0532109395964 & 0.909794134591 \\ \hline
        Yahoo & 0.205612368285 & 0.658273047868 \\ \hline
        Nvidia & 0.246641522789 & 0.59390682236 \\ \hline
        Netflix & -0.148248289524 & 0.751082336844 \\ \hline
    \end{tabular}
\caption{Pearson Coefficients analysis by company}
\label{coefficients-by-company}
\end{center}
\end{table}

As one can see from the table, there are high correlated companies such as Apple, Amazon or Research in Motion. However, there are also companies seemingly uncorrealted in sentiment analysis and stock prices, or even negatively correalted such as Microsoft or Google.

There might be several explanations for the result. First, our assumption is that product evaluation is crucial to the financial success to a company. However, it is not a practical assumption that the financial success is totally affected by only the competitiveness of products. Hence, product evaluation, which is a sampling toward product competitiveness will also not be the only factor that correlated with stock prices.

In addition, the Tweets we collected for study are also a sampling toward product evaluations. We collected the data basically by following some intuitive rules and we have not yet quantitatively verify the representiveness of this sampling. Besides, it is more reasonable to assume that the effect of product evaluation will take part in stock prices in the long term. However, due to some technical restrictions, we have collected only a few weeks as a proof-of-concept study.

Finally, we envision that there are quite a few rooms for the improvement for the accuracy of sentiment analysis. The accuracy in our study is around $75\%$ to $80\%$, which still have a fair amount of room for improvement.

However, we also want to note that the average of Pearson Coefficients from the study is $0.0537$, which means there is an overall slight positive correalation. If one incorporates our result to systems such as an automatic trading robot, the system may still benifited from the analysis we conducted here. In addition, we also believe that the positve correalation may be improved as we find more sophiscated mechanisms to handle the limitations mentioned above.

\subsection{Difference between Week Five and Week Seven}

Although the accuracy achieved by cross validation is higher in cross-validation (from around $76\%$ to around $79\%$), we found that the results from week five are more aligned with our expectation. For example, the positive ratio in week seven was around $80\%$ to $95\%$, which might be incorrect empirically speaking, if we take a closer look at individual Tweet in the corpus (i.e., randomly conduct a sampling and manually label the polarity). In contrast, in week five, the same metric was around $60\%$ to $80\%$, which is more aligned with our experience. In addition, the average Pearson correlation coefficient also dropped from $0.15$ to $0.05$.

The difference in the method we conducted the sentiment analysis is that we used the Random Forest as the classifier used for it yielded the best accuracy in terms of cross-validation on training set. Besides, more unigram features was used (from $244$ to $1043$). 

We observed that most of the Tweets will be classified as positive if objectivity test is ignored. In week seven, the results yielded a lower objectivity ratio from above $90\%$ to around $60\% ~ 80\%$. These factors combined might be the reason of higher positive ratios and hence the results that less aligned with our expectation.

We may use the method in week five to conduct the objectivity classification and use the method in week seven to conduct the polarity classification to verify whether it is the reason for higher positive ratios. We will also use more manually labelled data as ground truth in the future, instead of using only the training data collected by querying emoticons.

\section{Limitations}
We list some limitations here. First, we assumed that we can get positive tweets by querying ':)' or negative tweets by ':('. However, the semantic meaning in tweets can be complicated. Such as the following tweet by user @franki\_kuka: 'only the mail..? ;)) \#FF RT @asphodelia: \#Yahoo! Mail appears to be down...'. In this tweet, the user is probably being ironic. However, this will not be detected by our current system.

Collecting tweets once a week might miss some 'spike'. For instance, if an important product released at Wednesday but we happen to collect the data only at Saturday morning then we might miss all the discussions on that Wednesday. Theoretically we can collect all the tweets in realtime to overcome this problem. However, we are technically limited by storage and Twitter API usage limitations.

Current system doesn't handle new keywords that come out in middle of our study period. For instance, if iPad3 happen to release in this period, we will not include it in the current system. One way to solve it is to propose a better automatic keyword extraction mechanism.

The period of collecting tweets is not long enough. Ideally we should collect for at least one or two years of tweets so that we can catch more big event such as important product releases and also reduce the impact of short-term noise.

Although products of a company is its most important and fundamental factor to its revenue and thus influence its stock price. However, there are other factors that influence the price. For example, the plan that Apple decide how to use its huge amount of cash on March 19, 2012 also influence its price. However, we argue that our system can also take this category of news into account, which can be one of our future work.

\section{Conclusion and Future Directions}

We conducted sentiment analysis on Tweets from several technology companies. The correlation between the ratio of sentiment analysis and stock prices are examined. We discovered that there is an overall positive correlation between the two. Although not being extremely strong, this correlation can be used into systems such as algorithimic stock trading systems. Furthermore, it is our belief that as we handle the limitations and technical restrictions in a more sophiscated way, stronger correlation can be found.

We list some future directions here. First, better sentiment analysis techniques can be investigated. We have achieved accuracy that comparable to previous works. However, it is still far from perfect. Although there are high-level semantic meanings that might be extremely difficult to machines, empirically it occupies only a small portion among all the Tweets. Accuracy around $85\%$ to $90\%$ can be expected, theoretically. This means there are aroudn $10\%$ possible progress to be investigated for future sentiment analysis techniques.

A better sampling of product evaluation can also be achieved by incorporating better automatic keywords extraction techniques. One possible technique to use is the Twitter Streaming API \cite{TwtStrAPI} and listen to top keywords associated with our interested companies. The mechanism has the benefit that it can dynamically adjust the keywords to be used. Besides, there is no capacity restriction have been put on the API currently.

We would also like to study companies outside the technology industry. To be effective evaluated, these companies should have relatively discussed by more Tweets. We choose technology companies in this study because they tend to be discussed more on the Twitter, so that we can have a larger sample. However, we also observe that there are companies in other industries are widely discussed on the social network as well, such as Starbucks or Walmart. It would be interesting if we extend our research to these companies.

In this study we verified whether there is correlations between sentiment analysis and stock prices. Another direction that excites us even more is to use statistical method to predict the stock price given the ratio of sentiment analysis.

\bibliographystyle{abbrv}
\bibliography{reference}

\end{document}

