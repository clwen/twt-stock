\title{CSC 2511 Project: Predicting Stock Prices using Opnion Mining and Setiment Analysis of Twitter Data}
\author{
    Chung-Lin Wen \\
    Department of Computer Science \\
    University of Toronto
}
\date{\today}

\documentclass[12pt]{article}
\fontfamily{ptm}

\begin{document}
\maketitle

\input{defs}

\section{Introduction}
Nowadays, it is pretty common for consumers to post their evaluation for a product online, especially on social networks or blogs. It is highly possible that companies that receive more positive comments will yield a better performance better than companies that don't, since better customer evaluation imply better brand value and bigger revenue. For financial performance here, we will focus on stock price as the metric. Hence, we propose to verify that if there do exist a positive correlation between the consumer evaluations and the stock prices.

To verify our assumption, we will start with a few companies that are widely discussed over the web. Besides, ideally, its revenue should primarily depend on a few key products, which means we can use a few keywords to collect the data we need. For instance, \#iphone4s, \#iphone4, \#iphone3, \#ipad, \#macbookair, etc. Apple Incorporation, for instance, is one of the company we will start with, since it meets the condition we discussed above. 

For the social network platform we will use, we favor twitter as our major data source for the following reasons. First, the amount of data is huge enough for our usage. Second, its microblogging nature provide sentence level texts that focus on usually a single idea (I love \#iPhone4S coz I had a lot of fun playing with \#Siri), thus reducing the noise we have to deal with.

After collecting the data, we will classify the tweets into positive, neutral (or irrelevant), negative. After that, we will study the relationship between this evaluation and the stock price of corresponding companies using statistical techniques.

If time permitted, we would also like to apply the same study to other languages such as Chinese or Japanese and other companies such as HTC or Sony.

\section{Related Works}
For its wide and valuable usage, sentiment analysis, which is to classify the polarity (positive, negative) given a document or a sentence, has been widely studied yet still an extremely challenging problem. Pang and Lee \cite{Pang:08} did a comprehensive survey about the state-of-the-art techniques in the sentimental area. Liu \cite{Liu:10} summarizes the theory and techniques of the subfield for the purpose of teaching and learning.

Turney \etal~\cite{Turney:02} pioneered in the field by firstly address the problem. He first extract phrases that might contain subjective evaluation using its POS tags, then apply PMI-IR algorithm \cite{Turney:01} to determine the semantic orientation by measuring the similarity of the extracted phrases and "excellent", "poor" (regarded as positive and negative, respectively, in this paper). Finally, the polarity of a document is given by averaging the semantic orientations of all the extracted phrases.

In contrast, rather than using predefined bags of words for polarity, Pang \etal~\cite{Pang:02} approach the problem using features that are prior-knowledge-free. They use unigrams and bigrams extracted from corpus according to the frequencies then apply the standard machine learning classifiers, which including Naive Bayes, Maximum Entropy and Support Vector Machines (SVM). They concludes that using the presence of unigrams and using SVM as classfier yields the best accuracy (82.9\%).

Besides polarity, detection of opinion strength is also made posible by further research. Thelwall \etal~\cite{Thelwall:10} proposed to use a dictionary of sentiment words with associated strength measures as one of its feature to detect the opinion strength. Repeated letters, punctuation, emoticons are also used to make the system suitable for informal text.

Since the popularity of social networks, especially the microblogging has grown quickly in the recent years, there are several systems utilize these platforms as the corpus of sentiment analysis. Go \etal~\cite{Go:09} first experiment sentiment analysis on Twitter, the most popular microblogging platform. They basically follow the algorithm used as Pang \etal~\cite{Pang:02}. Unigram, Bigram, POS tags are used as features; Bayes, SVM, Maximum Entropy are used as classifier. They report that the combination of unigram and Bayes with mutual information feature selection method yields the best result, which is slightly different from Pang \etal~\cite{Pang:02}

Pak and Paroubek \cite{Pak:10} conduct a two-phase classification to improve the accuracy. Tweets are first divide to objective and subjective, then subjective tweets are further divide to positive and negative. For training data, they use pre-defined emoticons to get the subjective data, while retrieve tweets from popular newspaper accounts as objective data. In terms of features and classifiers, they follow a similar setup as Pang \etal~\cite{Pang:02} but also includes trigrams as one of the feature. They conclude that bigram yields the best result since it maintains a good balance between coverage and also able to catch sentiment expressions such as negation.

Barbosa and Feng \cite{Barbosa:10} propose propose new features to increase the accuracy of Twitter sentiment analysis. They propose the use of two additional sets of features. One of the sets is the meta features. In addition to POS tags, a word list with pre-defined polarity and subjectivity is used. Note that if the word is preceded by negative expression (e.g., not, never), the polarity will be reversed. The other set they propose is the use of tweets syntax feature, such as retweet, hashtags, reply, link.

Agarwal \etal~\cite{Agarwal:11} claimed they have ahiceved a better result by incorporating the tree kernel model, which is a tree of features such as stop words, target, URL, English words. They compared the tree kernel based model with unigram model and feature-based model from previous work and conclude that the tree kernel model outperforms the other two. Besides, they claimed that using random sampled streaming tweets instead of that searched by keywords introduces fewer biases.

There are different applications on data mining of Twitter or other social networks. Petrovic \etal~\cite{Petrovic:10} proposed a system to detect new event on Twitter, on which news propagate even quicker than the traditional news medias.

There are also applications for sentiment analysis, although not necessarily using Twitter data as a corpus. Besides only indicating the review is positive or negative, Kim \etal~\cite{Kim:06} proposed a system that also extract the reasons behind these positive and negative opnions by an maximum entropy model.

Blair-Goldensohn \etal~\cite{Blair:08} propose to incorporate the readily available information other than text itself to produce a better result. They have also integrated a summary of customer review, which includes different aspects (e.g., food, decor, service, etc in a restaurant) and the corresponding reasons customers love or hates a specific store or product.

Among all the applications, we are most interested about applications that utilize sentiment analysis or opnion mining to predict some metrics in the physical world. Asur and Huberman \cite{Asur:10} used Twitter data to predict the box-office revenues for movies. They study 24 movies released between November 2009 and February 2010 and conclude that the average tweet-rates (published tweets per hour) is statiscally correalated with its box-office revenue. They also report that sentiment content can improve the correalation although not significant as tweet-rates themselves.

There are also financial applications using sentiment analysis for predicting stock prices. Schumaker \etal~\cite{Schumaker:2009} propose a system, AZFinText, which can predict the stock price of a company that mentioned by a financial news. They asuume financial news will have impact on the decision made within stock buyers, and it will have immediate effect so that the price after 20 minutes the news is released can be predicted. They implemented the SVR Sequential Minimal Optimization function through WEKA \cite{Witten:2005}, while using proper nouns and results of sentiment analysis as features. They conclude that both subjectivety and polarity contribute to the accuracy of stock prices prediction.

\section{Experiment Setup}
\subsection{Overview}
In this section, we will describe how we set up our experiment. Our experiment contains mainly two parts: sentiment analysis for tweets and statistical inference of relationship between sentiment analysis and stock prices. We first do a two-phase (subjectivity and then polarity) sentiment analysis then we investigate whether the ratio of sentiment analysis are positive correalated to stock prices or not by using statistical method. For sentiment analysis, it can be further decompose into the following modules: data collection, preprocessing, feature extraction and classification. 

We will detail each module by a subsection in the following of this section. In subsection \ref{data-collection} we will discuss how our training data and test data are defined and collecetd. In subsection \ref{preprocessing} we will discuss what kind of preprocessing we will apply to our data. Feature definition and selection will be discussed in subsection \ref{feature-extraction}. Then we detail the classification process in subsection \ref{classification}. Finally, we discuss the statiscal method to be used for relationship between sentiment analysis and stock prices in subsection \ref{statistical-inference}.

\subsection{Data Collection}\label{data-collection}
Our data can be divided into the following portions: test data, which contain tweets about products or companies we are interested; training data, which can be further divide to positive training data, negative training data, objective training data. We will first discuss the methodolgy we collect the data, then define our test data and training data subsequently.

\subsubsection{Methodology}
For the reasons stated before, we will mainly use Twitter as our primary corpus. Ideally, we would like to collect tweets back to one or two years to catch the trends that whenever a popular comes to the market, the stock prices soars (such as iPhone4), or reversely, whenever a product comes to the market but not fully satisfied consumer's expection, the stock price plump (such as iPhone4S). 

However, Since Twitter API only allow external users to parse the tweets no longer than a week from the time the data is requested, we will collect the tweets every week from week of Feburuary 13th to the end of this project. We write a script that will automatically fetch the tweets defined by our keywords detailed below in the same time of the week (Saturday morning in our case) to minimize the bias possibly introduced by collecting tweets at different time of a week. For instance, people might feel blue at Monday while happier at Friday night. We also note that it is impractical to get data older than one week by crawl the data directly from Twitter website instead of using the API since the tweets displayed on the website are also not older than one week by our observation, which might due to the technical convinence of Twitter.

The format of data fetched from Twitter API is JSON. We use simplejson of Python to parse the data and extract the text part and time stamp of each tweet.

\subsubsection{Test Data}
We fetch our test data by querying Twitter API with predefined query tems, which are products or keywords we are interested in. For each companies covered in this study, several keywords are associated as the method we will described later. We includes top tech companies that has sufficient historical stock prices data at NASDAQ. We prefer the consumer-facing companies such as Apple for we can expect there are more discussions on the Internet and hence more meaningful information can be extract. In contrast, we will not include companies that are not mainly targeted at normal consumer, such as IBM. We also note that companies that haven't IPO or just IPO are not included, such as Twitter itself or Facebook.

For keywords, we interested in the major products of the company (such as Windows for Microsoft) or new product that are widely discussed in Twitter (such as iPhone4S for Apple). We exclude some keywords that will introduce ambiguity (such as Microsoft Office with an literarily office). We do it by querying the company at Twitter web interface, and observe the keywords that are frequenly appear with the company. For companies that don't have apprent keyword (such as Amazon) to use, we just list the company name as its keyword. In addition, to have a broad coverage and get adapt to the casual nature of Twitter, we also listed some common abbreviations for products (such as 'gmap' stands for 'googlemap'). Note that a company might be highly correalated with keywords from other companies, such as 'Amazon' and 'iTunes'. We count only the keyword directly related to the current company.

Table \ref{companies-listing} lists the tech companies we would like to study, along with their key products and hash tags that we will use for searching in the Twitter. 

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Apple &  \vbox{\hbox{\strut \#apple, \#iphone, \#iphone4s, \#iphone4,}\hbox{\strut \#siri, \#ipod, \#mac, \#itunes}} \\ \hline
        Google & \vbox{\hbox{\strut \#google, \#android, \#gmail, \#youtube,}\hbox{\strut \#googleplus, \#gplus, \#googlemap, \#gmap}} \\ \hline
        Microsoft & \#windows, \#xbox, \#kinect, \#msn \\ \hline
        Amazon & \#amazon \\ \hline
        Research in Motion & \#rim, \#blackberry, \#bb \\ \hline
        Dell & \#dell \\ \hline
        Intel & \#intel, \#xeon, \#ultrabook \\ \hline
        Yahoo & \#yahoo, \#yahoomail, \#ymail, \#yim \\ \hline
        Nvidia & \#nvidia, \#geforce \\ \hline
        Netflix & \#netflix \\
        \hline
    \end{tabular}
\caption{Companies studied in this project and the associated keywords}
\label{companies-listing}
\end{center}
\end{table}

\subsubsection{Training Data}
To train the positive, negative, we fetch tweets with positve emoticons such as ':)' and negative emoticons such as ':(', respectively. Note that we didn't use other query term to minimize the bias possibly introduced by query term. 

For objective tweets, we fetch tweets from technology news account since our major interest is tweets about technology. We list the technology news account we used here: @TechCrunch, @CNETNews, @engadget, @slashdot, @RWW, @mashable, @Gizmodo, @gigaom, @allthingsd, @TheNextWeb, @verge, @Wired, @nytimesbits, @WSJTech, @SAI, @guardiantech, @HuffPostTech.

Additionally, we will also use the JDPA \cite{JDPA}, which is an annually annotated blog entries about automobiles and cameras, to train our ngram model.

\subsection{Preprocessing}\label{preprocessing}
For data normalization, we asuume URLs and Twitter-specific features might good features while classifying tweets, we replace all urls to 'URL', hashtag to 'HASHTAG' and reply to 'REPLY'. Afterwards we do tokenization so that each token, including punctuations are splitted by space. Finally, we conduct part-of-speech tagging using NLPlib \cite{NLPlib} by Mark Watson and Jason Wiener.

\subsection{Feature Extraction}\label{feature-extraction}
Our features can be divided into four categories: count of part-of-speech tags, meta-information, count of twitter-specific syntax, unigram model trained by our training data. For count of part-of-speech tags, we will count all the possible tags produced by NLPlib, although we suppose adjective or adverbs might carry more subjective information. We might remove some of relative useless features after initial analysis.

Meta-information includes average length of sentences, average length of tokens, number of sentences, etc., which forms our second category of features. Count of first/second/third person pronouns, punctuations will also be extracted.

We also count Twitter-specific features, which includes hashtags, replies. We listed features in previous three categories in Table \ref{feature-listing}.

Finally, we extract most used unigram in our positve and negative training data, respectively. Since Pang \etal~\cite{Pang:02} reported that examing only the presence rather than counting the frequency of unigram yields better result. We will experiment whether presence of frequency produce better accuracy. In addition, we will also examine the best range of number of features we should use from the trained model.

\begin{table}
\begin{center}
    \begin{tabular}{ | l || l | }
        \hline
        Part-of-speech tags &  \vbox{\hbox{\strut count of: CC, DD, DT, EX, FW, IN,}\hbox{\strut JJ, JJR, JJS, LS, MD,}\hbox{\strut NN, NNS, NNP, NNPS,}\hbox{\strut PDT, POS, PRP, PRP\$,}\hbox{\strut RB, RBR, RBS, RP, SYM, TO, UH,}\hbox{\strut VB, VBD, VBG, VBN, VBP, VBZ,}\hbox{\strut WDT, WP, WP\$, WRB}} \\ \hline
           Meta-information & \vbox{\hbox{\strut count of: first/second/thrid person pronouns}\hbox{\strut count of commas, colons and semi-colons}\hbox{\strut count of: dashes, parenthses, ellipses}\hbox{\strut count of: wh-words, slang acronyms, words all in upper case}\hbox{\strut average length of sentences (in tokens)}\hbox{\strut average length of tokens (exclude punctuations tokens)}\hbox{\strut number of sentences}} \\ \hline
        Twitter-specific syntax & count of reply, hashtags \\ \hline
        Ngram models & unigram trained from training data (frequency or presence) \\ \hline
    \end{tabular}
\caption{Features used in classification}
\label{feature-listing}
\end{center}
\end{table}

\subsection{Classification}\label{classification}
For classification, we will use Naive Bayes, Support Vector Machine (SVM) and Maimum Entropy through machine learning package WEKA \cite{Witten:2005}. We will evaluate the accuracy by conducting cross-validation on training data. We will then apply the classifier to our test data. As Pang \etal~\cite{Pang:02} and Go \etal~\cite{Go:09} reported, an accuracy around 80\% for polarity classfier can be expected.

We will first classify the tweets into subjective and objective. For subjective tweets, we further classify it to positve or negative. Hence, in the end of classification stage, for each company $c_i$, we will have positive ratio $p_i$, negative ratio $n_i$ and objective ratio $b_i$, where $p_i + n_i + b_i = 1$.

\subsection{Statistical Inference}\label{statistical-inference}
Finally, we will use statstical techniques to evaluate the correlation between consumer opinions and the stock prices. We will fetch stock prices history from Yahoo Finance for the companies we studied.

Our hypothesis is that the companies that receive more positive opinion will eventually get a positive impact on their stock prices. Hence, for a given company, we define the opinion vector as the difference of ratio between positive and negative opnion which represent the postive opnion toward a company $O = [o_i]$, where $o_i = p_i - n_i$, for $i$ from $1$ to $n$. We also have stock prices $S = [s_i]$, for $i$ from $1$ to $n$. The problem we are interested is that whether these two time series are correlated or not. We will use Pearson product-moment correlation coefficient (Pearson coefficient) to verify our hypothesis.

We define our Pearson coefficient as:
$$
PPMCC = \frac{\sum_{i=1}^{n}(O_i - \bar{O})(S_i - \bar{S})}{\sqrt{\sum_{i=1}^{n}(O_i - \bar{O})^2}\sqrt{\sum_{i=1}^{n}(S_i - \bar{S})^2}}
$$

Where $\bar{O}$ and $\bar{S}$ is the average over the opinion vector and the stock price, respectively.

Since we anticipate there might be a delaying effect from consumer evaluations to the time stock prices reflect the consumer opinion, we will use the cross correlation to find the best time window that maximize the Pearson coefficient. We will take that maximized Pearson coefficient to interpret our result.

The value of Pearson coefficient ranges from $-1$ to $1$. In variables such as stock prices that are influenced by complicated factors, a Pearson coefficient that larger than $0.5$ indicates a strong correlation, while a Pearson coefficient ranges from $0.1$ to $0.3$ can be considered as a minor correlation \cite{Cohen:1988}. We expect the Pearson coefficient of our experiment should be at least $0.1$, a lower bound of minor correlation.

Another statistical method we would like to use is the coherence of the cross-spectrum. The details will be updated later.

For the implementation of the statistical inference, we will use Python library SciPy \cite{SciPy}, NumPy \cite{NumPy} or RPy \cite{RPy}.

\section{Limitations}
Here we list some limitations. First, we assume that we can get positive tweets by querying ':)' or negative tweets by ':('. However, the semantic meaning in tweets can be complicated. Such as the following tweet by user @franki\_kuka: 'only the mail..? ;)) \#FF RT @asphodelia: \#Yahoo! Mail appears to be down...'. In this tweet, the user is probably being ironic. However, this will not be detected by our current system.

Second, collecting tweets once a week might miss some 'spike'. For instance, if an important product released at Wednesday but we happend to collect the data at Saturday morning then we might miss all the discussions on that Wednesday. Theoretically we can collect all the tweets in realtime to overcome this problem. However, we are technically limited by storage and Twitter API usage limitation.

Third, current system doesn't handle new keywords that come out in middle of our study period. For instance, if iPad3 happen to release in this period, we will not include it in the current system. One way to solve it is to propose a better automatic keyword extraction mechanism.

Finally, the period of collecting tweets is not long enough. Ideally we should collect for at least one or two years of tweets so that we can catch more big event such as important product releases and also reduce the impact of short-term noise.

\bibliographystyle{abbrv}
\bibliography{reference}

\end{document}

